{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "import os,sys\n",
    "import glob\n",
    "attractionList = ['sana', 'nayeon', 'momo']\n",
    "attractionFolderList = ['sana1', 'nayeon1', 'momo1']\n",
    "for attractionFolder in attractionFolderList:\n",
    "    image_dir = \"./data/\"+attractionFolder+\"/\"\n",
    "    target_resize_dir = \"./data_resize/\"\n",
    "    target_rotate_dir = \"./data_rotate/\"\n",
    "    if not os.path.isdir(target_resize_dir):\n",
    "        os.makedirs(target_resize_dir)\n",
    "    if not os.path.isdir(target_rotate_dir):\n",
    "        os.makedirs(target_rotate_dir)\n",
    "    files = glob.glob(image_dir+\"/*.*\")\n",
    "    print(image_dir)\n",
    "    print(target_resize_dir)\n",
    "    print(target_rotate_dir)\n",
    "    print(files[0])\n",
    "    print(files[0].split(\"/\"))\n",
    "    print(files[0].split(\"/\")[-1])\n",
    "    print(file.split(\"/\")[-1].split(\".\")[0])\n",
    "    print(len(files))\n",
    "    count = 1;\n",
    "    size = (256, 256)\n",
    "    for file in files:\n",
    "        im = Image.open(file)\n",
    "        im = im.convert('RGB')\n",
    "        print(\"i: \", count, im.format, im.size, im.mode, file.split(\"/\")[-1])\n",
    "        count+=1\n",
    "        im = ImageOps.fit(im, size, Image.ANTIALIAS, 0, (0.5, 0.5))\n",
    "        path = target_resize_dir+file.split(\"/\")[-1].split(\".\")[0]\n",
    "        print(\"path: \" + path)\n",
    "        im.save(path+\".jpg\", quality=100)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(30, (3, 3), strides=(1, 1), padding='same',\n",
    "                 activation='relu', input_shape=(180,180,3), \n",
    "                 name='conv1'))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=(3, 3), padding='same', \n",
    "                 name='pool1'))\n",
    "model.add(layers.Conv2D(60, (3, 3), activation='relu', \n",
    "                 name='conv2'))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', \n",
    "                 name='pool2'))\n",
    "model.add(layers.Conv2D(90, (3, 3), activation='relu', \n",
    "                 name='conv3'))\n",
    "model.add(layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same', \n",
    "                 name='pool3'))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(lr=0.1),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "# train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    " \n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=10,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.7,\n",
    "                                   zoom_range=[0.9, 2.2],\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='nearest',\n",
    "                                   validation_split=0.2)\n",
    " \n",
    "training_set = train_datagen.flow_from_directory('./data',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=13,\n",
    "                                                 target_size = (180, 180),\n",
    "                                                 batch_size = 2,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 subset=\"training\")\n",
    "validation_set = train_datagen.flow_from_directory('./data',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=13,\n",
    "                                                 target_size = (180, 180),\n",
    "                                                 batch_size = 2,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 subset=\"validation\")\n",
    "#early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "csv_logger = CSVLogger('./log.csv', append=True, separator=';')\n",
    "\n",
    "hist = model.fit_generator(training_set,\n",
    "                         steps_per_epoch = 20,\n",
    "                         epochs = 200,\n",
    "                         validation_data = validation_set,\n",
    "                         validation_steps = 10,\n",
    "                         callbacks=[csv_logger])\n",
    "from keras.models import load_model\n",
    " \n",
    "model.save('cnn_attraction_keras_model.h5')\n",
    " \n",
    "# output = classifier.predict_generator(test_set, steps=5)\n",
    "# print(test_set.class_indices)\n",
    "# print(output)\n",
    " \n",
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    " \n",
    "scores = model.evaluate_generator(\n",
    "            validation_set,\n",
    "            steps = 10)\n",
    " \n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    " \n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    " \n",
    "output = model.predict_generator(\n",
    "            validation_set,\n",
    "            steps = 10)\n",
    "print(validation_set.class_indices)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    " \n",
    "print(output)\n",
    "print(validation_set.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import CSVLogger\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, utils\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (48, 48), padding='same', use_bias=False, input_shape=(128, 256, 3)))\n",
    "model.add(BatchNormalization(axis=3, scale=False))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    " \n",
    "model.add(Conv2D(32, (24, 24), padding='same', use_bias=False))\n",
    "model.add(BatchNormalization(axis=3, scale=False))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (12, 12), padding='same', use_bias=False))\n",
    "model.add(BatchNormalization(axis=3, scale=False))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "model.add(Conv2D(128, (6, 6), padding='same', use_bias=False))\n",
    "model.add(BatchNormalization(axis=3, scale=False))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#sgd = optimizers.SGD(Ir=0.01, decay=1e-6, momentum=0.9, nestrov=True)\n",
    "# Compiling the CNN\n",
    "model.compile(optimizer = 'Adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    " \n",
    " \n",
    "# Part 2 - Fitting the CNN to the images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    " \n",
    "# train_datagen = ImageDataGenerator(rescale = 1./255)\n",
    " \n",
    "# 데이터셋 불러오기\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range=10,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   shear_range=0.7,\n",
    "                                   zoom_range=[0.9, 2.2],\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   fill_mode='nearest',\n",
    "                                   validation_split=0.2)\n",
    " \n",
    "training_set = train_datagen.flow_from_directory('./data',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=13,\n",
    "                                                 target_size = (256, 256),\n",
    "                                                 batch_size = 2,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 subset=\"training\")\n",
    "validation_set = train_datagen.flow_from_directory('./data',\n",
    "                                                 shuffle=True,\n",
    "                                                 seed=13,\n",
    "                                                 target_size = (256, 256),\n",
    "                                                 batch_size = 2,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 subset=\"validation\")\n",
    "#early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=40)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "csv_logger = CSVLogger('./log.csv', append=True, separator=';')\n",
    "\n",
    "hist = model.fit_generator(training_set,\n",
    "                         steps_per_epoch = 20,\n",
    "                         epochs = 200,\n",
    "                         validation_data = validation_set,\n",
    "                         validation_steps = 10,\n",
    "                         callbacks=[csv_logger])\n",
    "from keras.models import load_model\n",
    " \n",
    "model.save('cnn_attraction_keras_model.h5')\n",
    " \n",
    "# output = classifier.predict_generator(test_set, steps=5)\n",
    "# print(test_set.class_indices)\n",
    "# print(output)\n",
    " \n",
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    " \n",
    "scores = model.evaluate_generator(\n",
    "            validation_set,\n",
    "            steps = 10)\n",
    " \n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    " \n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    " \n",
    "output = model.predict_generator(\n",
    "            validation_set,\n",
    "            steps = 10)\n",
    "print(validation_set.class_indices)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    " \n",
    "print(output)\n",
    "print(validation_set.filenames)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('./cnn_attraction_keras_model.h5')\n",
    "\n",
    "# 모델 평가하기\n",
    "print(\"-- Evaluate --\")\n",
    " \n",
    "scores = model.evaluate_generator(\n",
    "            validation_set,\n",
    "            steps = 10)\n",
    " \n",
    "print(\"%s: %.2f%%\" %(model.metrics_names[1], scores[1]*100))\n",
    " \n",
    "# 모델 예측하기\n",
    "print(\"-- Predict --\")\n",
    " \n",
    "output = model.predict_generator(\n",
    "            validation_set,\n",
    "            steps = 10)\n",
    "# print(output)\n",
    "# print(validation_set.class_indices)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    " \n",
    "# print(validation_set.filenames)\n",
    " \n",
    " \n",
    "# 5. 학습과정 살펴보기\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "fig, loss_ax = plt.subplots()\n",
    " \n",
    "acc_ax = loss_ax.twinx()\n",
    " \n",
    "loss_ax.plot(hist.history['loss'], 'y', label='train loss')\n",
    "loss_ax.plot(hist.history['val_loss'], 'r', label='val loss')\n",
    "#loss_ax.set_ylim([0.0, 0.5])\n",
    " \n",
    "acc_ax.plot(hist.history['accuracy'], 'b', label='train acc')\n",
    "acc_ax.plot(hist.history['val_accuracy'], 'g', label='val acc')\n",
    "#acc_ax.set_ylim([0.8, 1.0])\n",
    " \n",
    "loss_ax.set_xlabel('epoch')\n",
    "loss_ax.set_ylabel('loss')\n",
    "acc_ax.set_ylabel('accuray')\n",
    " \n",
    "loss_ax.legend(loc='upper left')\n",
    "acc_ax.legend(loc='lower left')\n",
    " \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
